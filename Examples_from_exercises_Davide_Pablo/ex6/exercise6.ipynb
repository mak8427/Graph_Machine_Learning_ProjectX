{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b9533f0-c800-4daa-b8cb-505ed7f4df2b",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "Due:  Tue December 3, 8:00am"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44773582-50fe-4d66-ba8e-03a368b350ea",
   "metadata": {},
   "source": [
    "## GPS and Hyperparameters\n",
    "\n",
    "This exercise consists of two parts: first, you are to combine global transformer attention (from the last exercise) with message-passing (from the second exercise). It is completely up to you how you combine those aspects, alternating between the two seems to be one of the best available options though. You may use (pure) message-passing layers from pytorch-geometric for this exercise (but obviously not layers like GPSConv that already combine things - especially since GPSConv differs significantly from the architecture in the GPS paper...).\n",
    "\n",
    "The second part of the exercise is to find a good model (with hyperparameters) for peptides-func. For this task, I want you to use the tool weights&biases (wandb.ai) and their \"sweep\" functionality. You can find example code for this below. Since we do not have access to your wandb accounts, please provide screenshots of your results and verify that these models are indeed good.\n",
    "\n",
    "For the hyperparameter tuning, you must perform this on your hybrid architecture. It might be interesting to see in how far the results (which parameters are important etc) differ between pure transformers, pure message-passing (possibly with VN), and hybrid approaches, although such an evaluation is not necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33633a0d-3859-4d10-b3f4-d27c390a5e5c",
   "metadata": {},
   "source": [
    "## Hybrid GPS-like architecture"
   ]
  },
  {
   "cell_type": "code",
   "id": "dc8a2d74-2089-47eb-9f61-df4f8c2ee9d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T15:26:47.464381Z",
     "start_time": "2024-12-01T15:26:23.685947Z"
    }
   },
   "source": [
    "# your model code goes here\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric as pyg\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class GINELayerWithVN(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, edge_dim):\n",
    "        super(GINELayerWithVN, self).__init__(aggr='add')  # \"Add\" aggregation.\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(out_channels, out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(out_channels, out_channels)\n",
    "        )\n",
    "        self.edge_encoder = torch.nn.Linear(edge_dim, out_channels)\n",
    "        # Remove node_encoder from here\n",
    "        self.virtual_node_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(out_channels, out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(out_channels, out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.edge_encoder.weight)\n",
    "        for m in self.mlp:\n",
    "            if isinstance(m, torch.nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "        for m in self.virtual_node_mlp:\n",
    "            if isinstance(m, torch.nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, vn_embed, batch):\n",
    "        # x is already encoded via node_encoder in the main model\n",
    "        x = x.float()  # Ensure x is FloatTensor\n",
    "        edge_attr = edge_attr.float()  # Ensure edge_attr is FloatTensor\n",
    "        edge_attr = self.edge_encoder(edge_attr)\n",
    "\n",
    "        # Add virtual node embedding to node features\n",
    "        vn_expanded = vn_embed[batch]\n",
    "        x = x + vn_expanded\n",
    "\n",
    "        # Message Passing\n",
    "        out = self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "        # Update node embeddings\n",
    "        out = self.mlp(out)\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        # Compute messages\n",
    "        return x_j + edge_attr\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out\n",
    "\n",
    "# Laplacian Positional Encodings (LapPE)\n",
    "def compute_laplace_pe(data, num_eigenvec=10):\n",
    "    G = to_networkx(data, to_undirected=True)\n",
    "    A = nx.adjacency_matrix(G).astype(float)\n",
    "    num_nodes = A.shape[0]\n",
    "    D = np.diag(np.array(A.sum(axis=1)).flatten())\n",
    "    L = D - A.todense()\n",
    "    L = torch.tensor(L, dtype=torch.float, device=device)\n",
    "    try:\n",
    "        eigenvalues, eigenvectors = torch.linalg.eigh(L)\n",
    "    except RuntimeError:\n",
    "        eigenvalues, eigenvectors = torch.symeig(L, eigenvectors=True)\n",
    "    available_eigenvec = eigenvectors.shape[1] - 1\n",
    "    actual_num_eigenvec = min(num_eigenvec, available_eigenvec)\n",
    "    eigenvectors = eigenvectors[:, 1:1 + actual_num_eigenvec]\n",
    "    if actual_num_eigenvec < num_eigenvec:\n",
    "        pad_size = num_eigenvec - actual_num_eigenvec\n",
    "        padding = torch.zeros(eigenvectors.shape[0], pad_size, device=device)\n",
    "        eigenvectors = torch.cat([eigenvectors, padding], dim=1)\n",
    "    return eigenvectors  # Shape: (num_nodes, num_eigenvec)\n",
    "\n",
    "# Random Walk Structural Embeddings (RWSE)\n",
    "def compute_rwse(data, walk_length=10):\n",
    "    G = to_networkx(data, to_undirected=True)\n",
    "    A = nx.adjacency_matrix(G).astype(float)\n",
    "    A = A.todense()\n",
    "    num_nodes = A.shape[0]\n",
    "    A = torch.tensor(A, dtype=torch.float, device=device)\n",
    "    rw_features = []\n",
    "    A_power = A.clone()\n",
    "    for _ in range(walk_length):\n",
    "        diag = torch.diagonal(A_power)\n",
    "        rw_features.append(diag)\n",
    "        A_power = torch.matmul(A_power, A)\n",
    "    rwse = torch.stack(rw_features, dim=1)  # (num_nodes, walk_length)\n",
    "    return rwse  # Shape: (num_nodes, walk_length)\n",
    "\n",
    "# SignNet to ensure sign invariance\n",
    "class SignNet(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(SignNet, self).__init__()\n",
    "        self.phi = nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.phi(x) + self.phi(-x)\n",
    "\n",
    "# Graph Transformer Layer with Masking\n",
    "class GraphTransformerLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, num_heads=4, dropout=0.1):\n",
    "        super(GraphTransformerLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim=in_dim, num_heads=num_heads, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(in_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(out_dim, in_dim)\n",
    "        self.norm1 = nn.LayerNorm(in_dim)\n",
    "        self.norm2 = nn.LayerNorm(in_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, key_padding_mask=None):\n",
    "        # x: (sequence_length, batch_size, embed_dim)\n",
    "        attn_output, _ = self.self_attn(x, x, x, key_padding_mask=key_padding_mask)\n",
    "        x = x + attn_output\n",
    "        x = self.norm1(x)\n",
    "        linear_output = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "        x = x + linear_output\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "# Layer that combines Message Passing and Transformer\n",
    "class HybridLayer(nn.Module):\n",
    "    def __init__(self, mp_layer, transformer_layer):\n",
    "        super(HybridLayer, self).__init__()\n",
    "        self.mp_layer = mp_layer\n",
    "        self.transformer_layer = transformer_layer\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, vn_embed, batch):\n",
    "        # Message Passing Layer\n",
    "        x = self.mp_layer(x, edge_index, edge_attr, vn_embed, batch)\n",
    "        x = F.relu(x)\n",
    "        return x  # Return x to update vn_embed before transformer\n",
    "\n",
    "    def apply_transformer(self, x, batch):\n",
    "        # Prepare for Transformer Layer\n",
    "        x_padded, mask = pyg.utils.to_dense_batch(x, batch)  # x_padded: [batch_size, max_num_nodes, hidden_features]\n",
    "        x_padded = x_padded.transpose(0, 1)  # x_padded: [max_num_nodes, batch_size, hidden_features]\n",
    "        key_padding_mask = ~mask  # [batch_size, max_num_nodes]\n",
    "        x_padded = self.transformer_layer(x_padded, key_padding_mask=key_padding_mask)\n",
    "        x_padded = x_padded.transpose(0, 1)  # x_padded: [batch_size, max_num_nodes, hidden_features]\n",
    "        x = x_padded[mask]  # x: [num_nodes, hidden_features]\n",
    "        return x\n",
    "\n",
    "# Updated GNN Model with Virtual Node, GINE Layers, and Graph Transformer\n",
    "class GNNWithVirtualNodeAndGINEAndTransformer(torch.nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, edge_attr_dim, num_layers=5, lap_pe_dim=10, rwse_dim=10, num_heads=4):\n",
    "        super(GNNWithVirtualNodeAndGINEAndTransformer, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_features = hidden_features\n",
    "\n",
    "        # Node Encoder\n",
    "        self.node_encoder = nn.Linear(in_features, hidden_features)\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            mp_layer = GINELayerWithVN(\n",
    "                in_channels=hidden_features,\n",
    "                out_channels=hidden_features,\n",
    "                edge_dim=edge_attr_dim\n",
    "            )\n",
    "            transformer_layer = GraphTransformerLayer(\n",
    "                in_dim=hidden_features,\n",
    "                out_dim=hidden_features,\n",
    "                num_heads=num_heads\n",
    "            )\n",
    "            self.layers.append(HybridLayer(mp_layer, transformer_layer))\n",
    "\n",
    "        self.virtual_node_embedding = torch.nn.Embedding(1, hidden_features)\n",
    "        torch.nn.init.constant_(self.virtual_node_embedding.weight.data, 0)\n",
    "\n",
    "        self.mlp_virtual_node = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_features, hidden_features),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_features, hidden_features),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Positional Encodings\n",
    "        self.lap_pe_dim = lap_pe_dim\n",
    "        self.rwse_dim = rwse_dim\n",
    "        self.lap_pe_linear = nn.Linear(hidden_features, hidden_features)\n",
    "        self.rwse_linear = nn.Linear(rwse_dim, hidden_features)\n",
    "        self.signnet = SignNet(lap_pe_dim, hidden_features)\n",
    "\n",
    "        self.fc = torch.nn.Linear(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch, data):\n",
    "        # Apply node_encoder first\n",
    "        x = self.node_encoder(x)  # [num_nodes, hidden_features]\n",
    "        # Initialize positional encodings tensor\n",
    "        pos_enc = torch.zeros_like(x).to(device)  # [num_nodes, hidden_features]\n",
    "\n",
    "        # Use data.num_graphs to get the correct number of graphs\n",
    "        batch_size = data.num_graphs\n",
    "\n",
    "        # Initialize virtual node embedding\n",
    "        vn_embed = self.virtual_node_embedding.weight.repeat(batch_size, 1)  # [batch_size, hidden_features]\n",
    "\n",
    "        # Iterate over each graph in the batch\n",
    "        for graph_id in range(batch_size):\n",
    "            mask = (batch == graph_id)\n",
    "            node_idx = mask.nonzero(as_tuple=False).squeeze()\n",
    "\n",
    "            # Handle case when graph has no nodes\n",
    "            if node_idx.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            # Extract subgraph using pyg.utils.subgraph\n",
    "            sub_edge_index, sub_edge_attr = pyg.utils.subgraph(\n",
    "                node_idx,\n",
    "                edge_index,\n",
    "                edge_attr,\n",
    "                relabel_nodes=True,\n",
    "                num_nodes=x.size(0)\n",
    "            )\n",
    "\n",
    "            # Create sub_data\n",
    "            sub_data = pyg.data.Data(\n",
    "                x=x[node_idx],\n",
    "                edge_index=sub_edge_index,\n",
    "                edge_attr=sub_edge_attr\n",
    "            )\n",
    "\n",
    "            # Compute Positional Encodings for the sub-graph\n",
    "            lap_pe = compute_laplace_pe(sub_data, num_eigenvec=self.lap_pe_dim)\n",
    "            rwse = compute_rwse(sub_data, walk_length=self.rwse_dim)\n",
    "\n",
    "            # Apply SignNet to LapPE\n",
    "            lap_pe = self.signnet(lap_pe)  # [num_nodes_graph, hidden_features]\n",
    "\n",
    "            # Linear transformation\n",
    "            lap_pe = self.lap_pe_linear(lap_pe)  # [num_nodes_graph, hidden_features]\n",
    "            rwse = self.rwse_linear(rwse)        # [num_nodes_graph, hidden_features]\n",
    "\n",
    "            # Combine positional encodings\n",
    "            graph_pos_enc = lap_pe + rwse  # [num_nodes_graph, hidden_features]\n",
    "\n",
    "            # Assign to pos_enc\n",
    "            pos_enc[node_idx] = graph_pos_enc  # [num_nodes, hidden_features]\n",
    "\n",
    "        # Add positional encodings to node features\n",
    "        x = x + pos_enc  # [num_nodes, hidden_features]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            # Message Passing Layer\n",
    "            x = layer(x, edge_index, edge_attr, vn_embed, batch)\n",
    "\n",
    "            # Update virtual node embedding\n",
    "            vn_aggr = global_mean_pool(x, batch)  # [batch_size, hidden_features]\n",
    "            vn_embed = vn_embed + self.mlp_virtual_node(vn_aggr)  # [batch_size, hidden_features]\n",
    "\n",
    "            # Transformer Layer\n",
    "            x = layer.apply_transformer(x, batch)\n",
    "\n",
    "        # Apply global mean pooling\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_features]\n",
    "        x = self.fc(x)  # [batch_size, out_features]\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "625a8487-01eb-46c2-ace6-7211b900b406",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T15:26:47.468264Z",
     "start_time": "2024-12-01T15:26:47.465390Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "e53b0ec0-7936-4b9c-884f-b126418135f5",
   "metadata": {},
   "source": [
    "# WandB hyperparameter tuning example code"
   ]
  },
  {
   "cell_type": "code",
   "id": "10328b2a-634d-4c8e-93a2-ae255ae1b28a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T15:26:47.472880Z",
     "start_time": "2024-12-01T15:26:47.469272Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch_geometric as pyg\n",
    "import torch_scatter\n",
    "import copy"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "59a8fe3f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Before using wandb, you need to create an account. Then you can login by pasting your API key when prompted. (just the key, nothing else)"
   ]
  },
  {
   "cell_type": "code",
   "id": "aba8bfcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T15:26:52.443851Z",
     "start_time": "2024-12-01T15:26:47.473886Z"
    }
   },
   "source": [
    "import wandb\n",
    "wandb.login()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: mak84271. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "652f0fe4-eef7-4f8d-9b1a-93f66d72839a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T15:26:52.453818Z",
     "start_time": "2024-12-01T15:26:52.446861Z"
    }
   },
   "source": [
    "# find device\n",
    "if torch.cuda.is_available(): # NVIDIA\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available(): # apple M1/M2\n",
    "    device = torch.device('mps') \n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "device"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "f973d525",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T15:26:52.719070Z",
     "start_time": "2024-12-01T15:26:52.454826Z"
    }
   },
   "source": [
    "cora = pyg.datasets.Planetoid(root = \"dataset/cora\", name=\"Cora\")\n",
    "cora_graph = cora[0]\n",
    "cora_dense_adj = pyg.utils.to_dense_adj(cora_graph.edge_index).to(device)\n",
    "# cora_graph.x = cora_graph.x.unsqueeze(0) # Add an empty batch dimension. I needed that for compatibility with MolHIV later.\n",
    "cora_graph = cora_graph.to(device)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "fc7ea1c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T15:26:52.725279Z",
     "start_time": "2024-12-01T15:26:52.720124Z"
    }
   },
   "source": [
    "cora_graph.to(device)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "39becd27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T15:26:52.732364Z",
     "start_time": "2024-12-01T15:26:52.726285Z"
    }
   },
   "source": [
    "class GCNLayer(torch.nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, activation=torch.nn.functional.relu):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.W: torch.Tensor = torch.nn.Parameter(torch.zeros(in_features, out_features))\n",
    "        torch.nn.init.kaiming_normal_(self.W) \n",
    "\n",
    "    def forward(self, H: torch.Tensor, edge_index: torch.Tensor):\n",
    "        out = H.clone()\n",
    "        out += torch_scatter.scatter_add(H[edge_index[0]], edge_index[1], dim=0)\n",
    "        out = out.matmul(self.W)\n",
    "        if self.activation:\n",
    "            out = self.activation(out)\n",
    "        return out"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "baea114a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T15:26:52.738854Z",
     "start_time": "2024-12-01T15:26:52.733876Z"
    }
   },
   "source": [
    "def get_accuracy(model, cora, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(cora_graph.x, cora_graph.edge_index)\n",
    "    correct = (outputs[mask].argmax(-1) == cora_graph.y[mask]).sum()\n",
    "    return int(correct) / int(mask.sum())"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "a6b356e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T15:26:52.748021Z",
     "start_time": "2024-12-01T15:26:52.740927Z"
    }
   },
   "source": [
    "class GraphNet(torch.nn.Module):\n",
    "    def __init__(self, in_features:int, out_features:int, hidden_features:int, activation=torch.nn.functional.relu, dropout=0.1):\n",
    "        super(GraphNet, self).__init__()\n",
    "        self.activation = activation\n",
    "        if dropout>0:\n",
    "            self.dropout = torch.nn.Dropout(dropout)\n",
    "        else: \n",
    "            self.dropout = torch.nn.Identity()\n",
    "\n",
    "        self.layer_1 = GCNLayer(in_features=in_features, out_features=hidden_features)\n",
    "        self.layer_2 = GCNLayer(in_features=hidden_features, out_features=hidden_features, activation=self.activation)\n",
    "        self.layer_3 = GCNLayer(in_features=hidden_features, out_features=hidden_features, activation=self.activation)\n",
    "        self.dense1 = torch.nn.Linear(in_features=hidden_features, out_features=hidden_features)\n",
    "        self.dense2 = torch.nn.Linear(in_features=hidden_features, out_features=out_features)\n",
    "\n",
    "    def forward(self, H: torch.Tensor, edge_index: torch.Tensor):\n",
    "        out = self.layer_1(H, edge_index)\n",
    "        out = self.dropout(out)\n",
    "        out = self.layer_2(out, edge_index)\n",
    "        out = self.dropout(out)\n",
    "        H = self.layer_3(out, edge_index)\n",
    "        H = self.dropout(out)\n",
    "        out = self.dense1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.dense2(out)\n",
    "        # H = torch.softmax(H, dim=-1)\n",
    "        # out = torch.nn.functional.softmax(out, dim=1)\n",
    "        return out\n",
    "\n",
    "        "
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "38c03c9a",
   "metadata": {},
   "source": [
    "## WandB train function\n",
    "\n",
    "We make a few changes to our train function to enable wandb logging of hyperparameters and metrics. The train function is written to allow both manual runs and hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "id": "2071964d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T15:26:52.756387Z",
     "start_time": "2024-12-01T15:26:52.749032Z"
    }
   },
   "source": [
    "def train(config=None, project=None, notes=None):\n",
    "\n",
    "    with wandb.init(config=config, project=project, notes=notes): # Initialize a new wandb run\n",
    "        # By passing our config through wandb,\n",
    "        # a) it is automatically logged\n",
    "        # b) we can use wandb sweeps to optimize hyperparameters\n",
    "        config = wandb.config \n",
    "\n",
    "        model = GraphNet(\n",
    "            in_features=cora_graph.num_features, \n",
    "            out_features=cora.num_classes, \n",
    "            hidden_features=config.hidden_features, \n",
    "            dropout=config.dropout).to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.epochs, eta_min=0)\n",
    "        criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "        best_model = None\n",
    "        best_val_acc = 0\n",
    "        best_epoch = 0\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            \n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(cora_graph.x, cora_graph.edge_index) # we run on everything\n",
    "\n",
    "            loss = criterion(outputs[cora_graph.train_mask], cora_graph.y[cora_graph.train_mask]) # but only propagate the loss for the train labels\n",
    "            loss = criterion(outputs, data.y.float())\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step() # update parameters\n",
    "            scheduler.step() # update the learning rate once per epoch\n",
    "\n",
    "            val_acc = get_accuracy(model, cora_graph, cora_graph.val_mask)\n",
    "            wandb.log({\"val_acc\": val_acc, \"loss\": loss.item()})\n",
    "\n",
    "            if epoch % 10 == 0 and not wandb.run.sweep_id:\n",
    "                # Only print information on individual runs, not on sweeps\n",
    "                print(f\"Epoch {epoch}, Loss: {loss.item()}, Val accuracy: {val_acc}\")\n",
    "\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_epoch = epoch\n",
    "                best_model = copy.deepcopy(model)\n",
    "\n",
    "    return best_model, best_epoch, best_val_acc\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "b173ddcf",
   "metadata": {},
   "source": [
    "## Manual training runs\n",
    "\n",
    "With wandb, you can still manually run your training loop with different hyperparameters as you are used to."
   ]
  },
  {
   "cell_type": "code",
   "id": "5d47a66b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T15:26:58.769586Z",
     "start_time": "2024-12-01T15:26:52.757394Z"
    }
   },
   "source": [
    "best_model, best_model_epoch, best_val_acc = train(dict(\n",
    "    hidden_features=128,\n",
    "    lr=0.01,\n",
    "    dropout=0.1,\n",
    "    epochs=100\n",
    "), project=\"Cora_GraphNet\", notes=\"first trial\")\n",
    "\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\dadoi\\DataspellProjects\\Graph_Machine_Learning\\wandb\\run-20241201_162652-p4i5joe0</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mak84271/Cora_GraphNet/runs/p4i5joe0' target=\"_blank\">winter-oath-5</a></strong> to <a href='https://wandb.ai/mak84271/Cora_GraphNet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/mak84271/Cora_GraphNet' target=\"_blank\">https://wandb.ai/mak84271/Cora_GraphNet</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/mak84271/Cora_GraphNet/runs/p4i5joe0' target=\"_blank\">https://wandb.ai/mak84271/Cora_GraphNet/runs/p4i5joe0</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dadoi\\AppData\\Local\\Temp\\ipykernel_30132\\3896885779.py\", line 29, in train\n",
      "    loss = criterion(outputs[cora_graph.train_mask], cora_graph.y[cora_graph.train_mask]) # but only propagate the loss for the train labels\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 819, in forward\n",
      "    return F.binary_cross_entropy_with_logits(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3624, in binary_cross_entropy_with_logits\n",
      "    raise ValueError(\n",
      "ValueError: Target size (torch.Size([140])) must be the same as input size (torch.Size([140, 7]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='0.007 MB of 0.007 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "450172cc3d534402aba896be29659879"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">winter-oath-5</strong> at: <a href='https://wandb.ai/mak84271/Cora_GraphNet/runs/p4i5joe0' target=\"_blank\">https://wandb.ai/mak84271/Cora_GraphNet/runs/p4i5joe0</a><br/> View project at: <a href='https://wandb.ai/mak84271/Cora_GraphNet' target=\"_blank\">https://wandb.ai/mak84271/Cora_GraphNet</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241201_162652-p4i5joe0\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([140])) must be the same as input size (torch.Size([140, 7]))",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m best_model, best_model_epoch, best_val_acc \u001B[38;5;241m=\u001B[39m train(\u001B[38;5;28mdict\u001B[39m(\n\u001B[0;32m      2\u001B[0m     hidden_features\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m128\u001B[39m,\n\u001B[0;32m      3\u001B[0m     lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.01\u001B[39m,\n\u001B[0;32m      4\u001B[0m     dropout\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m,\n\u001B[0;32m      5\u001B[0m     epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m\n\u001B[0;32m      6\u001B[0m ), project\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCora_GraphNet\u001B[39m\u001B[38;5;124m\"\u001B[39m, notes\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfirst trial\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[10], line 29\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(config, project, notes)\u001B[0m\n\u001B[0;32m     26\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     27\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(cora_graph\u001B[38;5;241m.\u001B[39mx, cora_graph\u001B[38;5;241m.\u001B[39medge_index) \u001B[38;5;66;03m# we run on everything\u001B[39;00m\n\u001B[1;32m---> 29\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs[cora_graph\u001B[38;5;241m.\u001B[39mtrain_mask], cora_graph\u001B[38;5;241m.\u001B[39my[cora_graph\u001B[38;5;241m.\u001B[39mtrain_mask]) \u001B[38;5;66;03m# but only propagate the loss for the train labels\u001B[39;00m\n\u001B[0;32m     30\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, data\u001B[38;5;241m.\u001B[39my\u001B[38;5;241m.\u001B[39mfloat())\n\u001B[0;32m     31\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32mC:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mC:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:819\u001B[0m, in \u001B[0;36mBCEWithLogitsLoss.forward\u001B[1;34m(self, input, target)\u001B[0m\n\u001B[0;32m    818\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 819\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mbinary_cross_entropy_with_logits(\n\u001B[0;32m    820\u001B[0m         \u001B[38;5;28minput\u001B[39m,\n\u001B[0;32m    821\u001B[0m         target,\n\u001B[0;32m    822\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight,\n\u001B[0;32m    823\u001B[0m         pos_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpos_weight,\n\u001B[0;32m    824\u001B[0m         reduction\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreduction,\n\u001B[0;32m    825\u001B[0m     )\n",
      "File \u001B[1;32mC:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\functional.py:3624\u001B[0m, in \u001B[0;36mbinary_cross_entropy_with_logits\u001B[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001B[0m\n\u001B[0;32m   3621\u001B[0m     reduction_enum \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mget_enum(reduction)\n\u001B[0;32m   3623\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (target\u001B[38;5;241m.\u001B[39msize() \u001B[38;5;241m==\u001B[39m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize()):\n\u001B[1;32m-> 3624\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   3625\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTarget size (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtarget\u001B[38;5;241m.\u001B[39msize()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) must be the same as input size (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3626\u001B[0m     )\n\u001B[0;32m   3628\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mbinary_cross_entropy_with_logits(\n\u001B[0;32m   3629\u001B[0m     \u001B[38;5;28minput\u001B[39m, target, weight, pos_weight, reduction_enum\n\u001B[0;32m   3630\u001B[0m )\n",
      "\u001B[1;31mValueError\u001B[0m: Target size (torch.Size([140])) must be the same as input size (torch.Size([140, 7]))"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c5b9ca8b008be13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b0a0ad4db19a89dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cd346675",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T15:29:11.892811Z",
     "start_time": "2024-12-01T15:29:11.857003Z"
    }
   },
   "source": [
    "test_acc = get_accuracy(best_model, cora_graph, cora_graph.test_mask)\n",
    "print(f\"Test acc: {test_acc:.2f} (using model from epoch {best_model_epoch} with val acc {best_val_acc:.2})\")"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m test_acc \u001B[38;5;241m=\u001B[39m get_accuracy(best_model, cora_graph, cora_graph\u001B[38;5;241m.\u001B[39mtest_mask)\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest acc: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtest_acc\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (using model from epoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbest_model_epoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m with val acc \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbest_val_acc\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'best_model' is not defined"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "49baed50",
   "metadata": {},
   "source": [
    "## Hyperparameter Search\n",
    "\n",
    "But you can also perform a hyperparameter search using wandb sweeps, by specifying a hyperparameter config"
   ]
  },
  {
   "cell_type": "code",
   "id": "be78650c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T15:29:12.928780Z",
     "start_time": "2024-12-01T15:29:12.923237Z"
    }
   },
   "source": [
    "sweep_config = {\n",
    "    # hyperparameter search methods, e.g. grid, random\n",
    "    'method': 'random',\n",
    "\n",
    "    # metric to optimize\n",
    "    'metric': {\n",
    "        'name': 'val_acc',\n",
    "        'goal': 'maximize'   \n",
    "    },\n",
    "\n",
    "    # parameters to search\n",
    "    'parameters': {\n",
    "        'hidden_features': {\n",
    "            'values': [64, 128, 256]\n",
    "        },\n",
    "        'dropout': {\n",
    "            # a flat distribution between 0 and 0.1\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.0,\n",
    "            'max': 0.5,\n",
    "        },\n",
    "        'lr': {\n",
    "            'values': [0.001, 0.0001, 0.00001]\n",
    "        },\n",
    "        'epochs': {\n",
    "            'values': [100, 200, 300]\n",
    "        }\n",
    "    }\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "bdb6ca89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T15:29:14.123432Z",
     "start_time": "2024-12-01T15:29:13.484196Z"
    }
   },
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"Cora_GraphNet\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: iewx3aml\n",
      "Sweep URL: https://wandb.ai/mak84271/Cora_GraphNet/sweeps/iewx3aml\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "4f32d0c7",
   "metadata": {},
   "source": [
    "You can click on the `Sweep URL` to get a nice visualization on how well different sets of hyperparameters perform and to see which are the best (click on the best run and then on Overview).\n",
    "\n",
    "The following cell performs 5 runs using the sweep configuration given above. You can call `wandb.agent` multiple times to produce more runs for the same sweep configuration."
   ]
  },
  {
   "cell_type": "code",
   "id": "57e588c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T15:29:30.134572Z",
     "start_time": "2024-12-01T15:29:14.584092Z"
    }
   },
   "source": [
    "wandb.agent(sweep_id, function=train, count=5)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Agent Starting Run: 9k0ima7o with config:\n",
      "wandb: \tdropout: 0.21382270781919233\n",
      "wandb: \tepochs: 300\n",
      "wandb: \thidden_features: 64\n",
      "wandb: \tlr: 1e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\dadoi\\DataspellProjects\\Graph_Machine_Learning\\wandb\\run-20241201_162916-9k0ima7o</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mak84271/Cora_GraphNet/runs/9k0ima7o' target=\"_blank\">colorful-sweep-1</a></strong> to <a href='https://wandb.ai/mak84271/Cora_GraphNet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mak84271/Cora_GraphNet/sweeps/iewx3aml' target=\"_blank\">https://wandb.ai/mak84271/Cora_GraphNet/sweeps/iewx3aml</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/mak84271/Cora_GraphNet' target=\"_blank\">https://wandb.ai/mak84271/Cora_GraphNet</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View sweep at <a href='https://wandb.ai/mak84271/Cora_GraphNet/sweeps/iewx3aml' target=\"_blank\">https://wandb.ai/mak84271/Cora_GraphNet/sweeps/iewx3aml</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/mak84271/Cora_GraphNet/runs/9k0ima7o' target=\"_blank\">https://wandb.ai/mak84271/Cora_GraphNet/runs/9k0ima7o</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dadoi\\AppData\\Local\\Temp\\ipykernel_30132\\3896885779.py\", line 29, in train\n",
      "    loss = criterion(outputs[cora_graph.train_mask], cora_graph.y[cora_graph.train_mask]) # but only propagate the loss for the train labels\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 819, in forward\n",
      "    return F.binary_cross_entropy_with_logits(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3624, in binary_cross_entropy_with_logits\n",
      "    raise ValueError(\n",
      "ValueError: Target size (torch.Size([140])) must be the same as input size (torch.Size([140, 7]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">colorful-sweep-1</strong> at: <a href='https://wandb.ai/mak84271/Cora_GraphNet/runs/9k0ima7o' target=\"_blank\">https://wandb.ai/mak84271/Cora_GraphNet/runs/9k0ima7o</a><br/> View project at: <a href='https://wandb.ai/mak84271/Cora_GraphNet' target=\"_blank\">https://wandb.ai/mak84271/Cora_GraphNet</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241201_162916-9k0ima7o\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 9k0ima7o errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\wandb\\agents\\pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"C:\\Users\\dadoi\\AppData\\Local\\Temp\\ipykernel_30132\\3896885779.py\", line 29, in train\n",
      "    loss = criterion(outputs[cora_graph.train_mask], cora_graph.y[cora_graph.train_mask]) # but only propagate the loss for the train labels\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 819, in forward\n",
      "    return F.binary_cross_entropy_with_logits(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3624, in binary_cross_entropy_with_logits\n",
      "    raise ValueError(\n",
      "ValueError: Target size (torch.Size([140])) must be the same as input size (torch.Size([140, 7]))\n",
      "\n",
      "wandb: ERROR Run 9k0ima7o errored:\n",
      "wandb: ERROR Traceback (most recent call last):\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\wandb\\agents\\pyagent.py\", line 306, in _run_job\n",
      "wandb: ERROR     self._function()\n",
      "wandb: ERROR   File \"C:\\Users\\dadoi\\AppData\\Local\\Temp\\ipykernel_30132\\3896885779.py\", line 29, in train\n",
      "wandb: ERROR     loss = criterion(outputs[cora_graph.train_mask], cora_graph.y[cora_graph.train_mask]) # but only propagate the loss for the train labels\n",
      "wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "wandb: ERROR     return self._call_impl(*args, **kwargs)\n",
      "wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "wandb: ERROR     return forward_call(*args, **kwargs)\n",
      "wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 819, in forward\n",
      "wandb: ERROR     return F.binary_cross_entropy_with_logits(\n",
      "wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3624, in binary_cross_entropy_with_logits\n",
      "wandb: ERROR     raise ValueError(\n",
      "wandb: ERROR ValueError: Target size (torch.Size([140])) must be the same as input size (torch.Size([140, 7]))\n",
      "wandb: ERROR \n",
      "wandb: Agent Starting Run: yucwsa03 with config:\n",
      "wandb: \tdropout: 0.31594391419162104\n",
      "wandb: \tepochs: 100\n",
      "wandb: \thidden_features: 128\n",
      "wandb: \tlr: 0.0001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\dadoi\\DataspellProjects\\Graph_Machine_Learning\\wandb\\run-20241201_162921-yucwsa03</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mak84271/Cora_GraphNet/runs/yucwsa03' target=\"_blank\">rosy-sweep-2</a></strong> to <a href='https://wandb.ai/mak84271/Cora_GraphNet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mak84271/Cora_GraphNet/sweeps/iewx3aml' target=\"_blank\">https://wandb.ai/mak84271/Cora_GraphNet/sweeps/iewx3aml</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/mak84271/Cora_GraphNet' target=\"_blank\">https://wandb.ai/mak84271/Cora_GraphNet</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View sweep at <a href='https://wandb.ai/mak84271/Cora_GraphNet/sweeps/iewx3aml' target=\"_blank\">https://wandb.ai/mak84271/Cora_GraphNet/sweeps/iewx3aml</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/mak84271/Cora_GraphNet/runs/yucwsa03' target=\"_blank\">https://wandb.ai/mak84271/Cora_GraphNet/runs/yucwsa03</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dadoi\\AppData\\Local\\Temp\\ipykernel_30132\\3896885779.py\", line 29, in train\n",
      "    loss = criterion(outputs[cora_graph.train_mask], cora_graph.y[cora_graph.train_mask]) # but only propagate the loss for the train labels\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 819, in forward\n",
      "    return F.binary_cross_entropy_with_logits(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3624, in binary_cross_entropy_with_logits\n",
      "    raise ValueError(\n",
      "ValueError: Target size (torch.Size([140])) must be the same as input size (torch.Size([140, 7]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rosy-sweep-2</strong> at: <a href='https://wandb.ai/mak84271/Cora_GraphNet/runs/yucwsa03' target=\"_blank\">https://wandb.ai/mak84271/Cora_GraphNet/runs/yucwsa03</a><br/> View project at: <a href='https://wandb.ai/mak84271/Cora_GraphNet' target=\"_blank\">https://wandb.ai/mak84271/Cora_GraphNet</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241201_162921-yucwsa03\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run yucwsa03 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\wandb\\agents\\pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"C:\\Users\\dadoi\\AppData\\Local\\Temp\\ipykernel_30132\\3896885779.py\", line 29, in train\n",
      "    loss = criterion(outputs[cora_graph.train_mask], cora_graph.y[cora_graph.train_mask]) # but only propagate the loss for the train labels\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 819, in forward\n",
      "    return F.binary_cross_entropy_with_logits(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3624, in binary_cross_entropy_with_logits\n",
      "    raise ValueError(\n",
      "ValueError: Target size (torch.Size([140])) must be the same as input size (torch.Size([140, 7]))\n",
      "\n",
      "wandb: ERROR Run yucwsa03 errored:\n",
      "wandb: ERROR Traceback (most recent call last):\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\wandb\\agents\\pyagent.py\", line 306, in _run_job\n",
      "wandb: ERROR     self._function()\n",
      "wandb: ERROR   File \"C:\\Users\\dadoi\\AppData\\Local\\Temp\\ipykernel_30132\\3896885779.py\", line 29, in train\n",
      "wandb: ERROR     loss = criterion(outputs[cora_graph.train_mask], cora_graph.y[cora_graph.train_mask]) # but only propagate the loss for the train labels\n",
      "wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "wandb: ERROR     return self._call_impl(*args, **kwargs)\n",
      "wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "wandb: ERROR     return forward_call(*args, **kwargs)\n",
      "wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 819, in forward\n",
      "wandb: ERROR     return F.binary_cross_entropy_with_logits(\n",
      "wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3624, in binary_cross_entropy_with_logits\n",
      "wandb: ERROR     raise ValueError(\n",
      "wandb: ERROR ValueError: Target size (torch.Size([140])) must be the same as input size (torch.Size([140, 7]))\n",
      "wandb: ERROR \n",
      "wandb: Agent Starting Run: nbwpi8ii with config:\n",
      "wandb: \tdropout: 0.3921005521732607\n",
      "wandb: \tepochs: 100\n",
      "wandb: \thidden_features: 128\n",
      "wandb: \tlr: 0.001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\dadoi\\DataspellProjects\\Graph_Machine_Learning\\wandb\\run-20241201_162927-nbwpi8ii</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mak84271/Cora_GraphNet/runs/nbwpi8ii' target=\"_blank\">serene-sweep-3</a></strong> to <a href='https://wandb.ai/mak84271/Cora_GraphNet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mak84271/Cora_GraphNet/sweeps/iewx3aml' target=\"_blank\">https://wandb.ai/mak84271/Cora_GraphNet/sweeps/iewx3aml</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/mak84271/Cora_GraphNet' target=\"_blank\">https://wandb.ai/mak84271/Cora_GraphNet</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View sweep at <a href='https://wandb.ai/mak84271/Cora_GraphNet/sweeps/iewx3aml' target=\"_blank\">https://wandb.ai/mak84271/Cora_GraphNet/sweeps/iewx3aml</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/mak84271/Cora_GraphNet/runs/nbwpi8ii' target=\"_blank\">https://wandb.ai/mak84271/Cora_GraphNet/runs/nbwpi8ii</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dadoi\\AppData\\Local\\Temp\\ipykernel_30132\\3896885779.py\", line 29, in train\n",
      "    loss = criterion(outputs[cora_graph.train_mask], cora_graph.y[cora_graph.train_mask]) # but only propagate the loss for the train labels\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 819, in forward\n",
      "    return F.binary_cross_entropy_with_logits(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3624, in binary_cross_entropy_with_logits\n",
      "    raise ValueError(\n",
      "ValueError: Target size (torch.Size([140])) must be the same as input size (torch.Size([140, 7]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">serene-sweep-3</strong> at: <a href='https://wandb.ai/mak84271/Cora_GraphNet/runs/nbwpi8ii' target=\"_blank\">https://wandb.ai/mak84271/Cora_GraphNet/runs/nbwpi8ii</a><br/> View project at: <a href='https://wandb.ai/mak84271/Cora_GraphNet' target=\"_blank\">https://wandb.ai/mak84271/Cora_GraphNet</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241201_162927-nbwpi8ii\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run nbwpi8ii errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\wandb\\agents\\pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"C:\\Users\\dadoi\\AppData\\Local\\Temp\\ipykernel_30132\\3896885779.py\", line 29, in train\n",
      "    loss = criterion(outputs[cora_graph.train_mask], cora_graph.y[cora_graph.train_mask]) # but only propagate the loss for the train labels\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 819, in forward\n",
      "    return F.binary_cross_entropy_with_logits(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3624, in binary_cross_entropy_with_logits\n",
      "    raise ValueError(\n",
      "ValueError: Target size (torch.Size([140])) must be the same as input size (torch.Size([140, 7]))\n",
      "\n",
      "wandb: ERROR Run nbwpi8ii errored:\n",
      "wandb: ERROR Traceback (most recent call last):\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\wandb\\agents\\pyagent.py\", line 306, in _run_job\n",
      "wandb: ERROR     self._function()\n",
      "wandb: ERROR   File \"C:\\Users\\dadoi\\AppData\\Local\\Temp\\ipykernel_30132\\3896885779.py\", line 29, in train\n",
      "wandb: ERROR     loss = criterion(outputs[cora_graph.train_mask], cora_graph.y[cora_graph.train_mask]) # but only propagate the loss for the train labels\n",
      "wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "wandb: ERROR     return self._call_impl(*args, **kwargs)\n",
      "wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "wandb: ERROR     return forward_call(*args, **kwargs)\n",
      "wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 819, in forward\n",
      "wandb: ERROR     return F.binary_cross_entropy_with_logits(\n",
      "wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3624, in binary_cross_entropy_with_logits\n",
      "wandb: ERROR     raise ValueError(\n",
      "wandb: ERROR ValueError: Target size (torch.Size([140])) must be the same as input size (torch.Size([140, 7]))\n",
      "wandb: ERROR \n",
      "Detected 3 failed runs in the first 60 seconds, killing sweep.\n",
      "wandb: ERROR Detected 3 failed runs in the first 60 seconds, killing sweep.\n",
      "wandb: To disable this check set WANDB_AGENT_DISABLE_FLAPPING=true\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "fb03889b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T15:29:30.146984Z",
     "start_time": "2024-12-01T15:29:30.136635Z"
    }
   },
   "source": [
    "# Close the sweep, otherwise individual runs after the sweep will still be logged as part of it\n",
    "wandb.teardown() "
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Peptide dataset",
   "id": "39370cb2b090800b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T15:29:31.896638Z",
     "start_time": "2024-12-01T15:29:30.147989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric as pyg\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import LRGBDataset\n",
    "import wandb\n",
    "import copy\n",
    "\n",
    "\n",
    "# Detect device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Load the peptides-func dataset\n",
    "# Define a transform function\n",
    "def to_float(data):\n",
    "    data.x = data.x.float()\n",
    "    data.edge_attr = data.edge_attr.float()\n",
    "    return data\n",
    "\n",
    "# Load the dataset with the transform\n",
    "dataset = LRGBDataset(root='dataset/peptides-func', name='Peptides-func', transform=to_float)\n",
    "\n",
    "print(f'Dataset size: {len(dataset)}')\n",
    "\n",
    "# Determine the number of node features, edge features, and classes\n",
    "num_node_features = dataset.num_features\n",
    "num_edge_features = dataset[0].edge_attr.shape[1]\n",
    "num_classes = dataset.num_classes\n",
    "\n",
    "# Shuffle the dataset\n",
    "torch.manual_seed(42)\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "# Split the dataset\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "num_total = len(dataset)\n",
    "num_train = int(num_total * train_ratio)\n",
    "num_val = int(num_total * val_ratio)\n",
    "\n",
    "train_dataset = dataset[:num_train]\n",
    "val_dataset = dataset[num_train:num_train + num_val]\n",
    "test_dataset = dataset[num_train + num_val:]\n",
    "\n",
    "print(f'Train graphs: {len(train_dataset)}')\n",
    "print(f'Validation graphs: {len(val_dataset)}')\n",
    "print(f'Test graphs: {len(test_dataset)}')\n",
    "\n",
    "# Create data loaders\n",
    "def create_data_loader(dataset, batch_size, shuffle):\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "\n",
    "# We will create data loaders inside the train function\n",
    "# Your model class\n",
    "GraphNet = GNNWithVirtualNodeAndGINEAndTransformer\n",
    "\n"
   ],
   "id": "cbd5fcc05d90fdbc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Dataset size: 10873\n",
      "Train graphs: 8698\n",
      "Validation graphs: 1087\n",
      "Test graphs: 1088\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T15:29:31.908276Z",
     "start_time": "2024-12-01T15:29:31.898646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(config=None, project=None, notes=None):\n",
    "    with wandb.init(config=config, project=project, notes=notes):\n",
    "        config = wandb.config\n",
    "\n",
    "        model = GraphNet(\n",
    "            in_features=num_node_features,\n",
    "            hidden_features=config.hidden_features,\n",
    "            out_features=10,  # Number of tasks\n",
    "            edge_attr_dim=num_edge_features,\n",
    "            num_layers=config.num_layers,\n",
    "            lap_pe_dim=config.lap_pe_dim,\n",
    "            rwse_dim=config.rwse_dim,\n",
    "            num_heads=config.num_heads\n",
    "        ).to(device)\n",
    "\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.epochs, eta_min=0)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        best_model = None\n",
    "        best_val_acc = 0\n",
    "        best_epoch = 0\n",
    "\n",
    "        # Create data loaders\n",
    "        train_loader = create_data_loader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "        val_loader = create_data_loader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "        test_loader = create_data_loader(test_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            for data in train_loader:\n",
    "                data = data.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(data.x, data.edge_index, data.edge_attr, data.batch, data)\n",
    "                loss = criterion(outputs, data.y.squeeze())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            val_acc = evaluate(model, val_loader)\n",
    "            wandb.log({\"val_acc\": val_acc, \"loss\": total_loss / len(train_loader)})\n",
    "\n",
    "            if epoch % 10 == 0 and not wandb.run.sweep_id:\n",
    "                print(f\"Epoch {epoch}, Loss: {total_loss / len(train_loader):.4f}, Val accuracy: {val_acc:.4f}\")\n",
    "\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_epoch = epoch\n",
    "                best_model = copy.deepcopy(model)\n",
    "\n",
    "        # Evaluate on test set\n",
    "        test_acc = evaluate(best_model, test_loader)\n",
    "        wandb.log({\"test_acc\": test_acc})\n",
    "\n",
    "        print(f\"Best Epoch: {best_epoch}, Best Validation Accuracy: {best_val_acc:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "        return best_model, best_epoch, best_val_acc, test_acc\n"
   ],
   "id": "b61567a17ff43090",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T15:29:34.516232Z",
     "start_time": "2024-12-01T15:29:31.910794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            outputs = model(data.x, data.edge_index, data.edge_attr, data.batch, data)\n",
    "            y_true.append(data.y.cpu())\n",
    "            y_pred.append(outputs.cpu())\n",
    "    y_true = torch.cat(y_true, dim=0).numpy()\n",
    "    y_pred = torch.cat(y_pred, dim=0).numpy()\n",
    "    # Compute average ROC-AUC over tasks\n",
    "    roc_list = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        if np.sum(y_true[:, i]) == 0 or np.sum(y_true[:, i]) == y_true.shape[0]:\n",
    "            # Skip tasks with only one class present\n",
    "            continue\n",
    "        roc = roc_auc_score(y_true[:, i], y_pred[:, i])\n",
    "        roc_list.append(roc)\n",
    "    if len(roc_list) == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return sum(roc_list) / len(roc_list)\n",
    "\n"
   ],
   "id": "64b7de26f73826c",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T15:29:35.470928Z",
     "start_time": "2024-12-01T15:29:34.518236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {'name': 'val_roc_auc', 'goal': 'maximize'},\n",
    "    'parameters': {\n",
    "        'hidden_features': {'values': [64, 128, 256]},\n",
    "        'num_layers': {'values': [3, 5, 7]},\n",
    "        'num_heads': {'values': [4, 8]},\n",
    "        'lap_pe_dim': {'values': [5, 10, 15]},\n",
    "        'rwse_dim': {'values': [5, 10, 15]},\n",
    "        'dropout': {'min': 0.0, 'max': 0.5},\n",
    "        'lr': {'min': 1e-4, 'max': 1e-2, 'distribution': 'log_uniform'},\n",
    "        'weight_decay': {'min': 0.0, 'max': 1e-4},\n",
    "        'batch_size': {'values': [16, 32, 64]},\n",
    "        'epochs': {'value': 100}\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project='peptides-func-hyperparameter-tuning')\n"
   ],
   "id": "4b47e3c6688bb1cc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Malformed sweep config detected! This may cause your sweep to behave in unexpected ways.\n",
      "wandb: WARNING To avoid this, please fix the sweep config schema violations below:\n",
      "wandb: WARNING   Violation 1. lr uses log_uniform, where min/max specify base-e exponents. Use log_uniform_values to specify limit values.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: yw5hts1u\n",
      "Sweep URL: https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/sweeps/yw5hts1u\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T15:32:35.633670Z",
     "start_time": "2024-12-01T15:29:35.472937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wandb.login()  # Ensure you are logged in to WandB\n",
    "\n",
    "# Start the sweep agent\n",
    "wandb.agent(sweep_id, function=train)\n"
   ],
   "id": "3c5256d47b2847c3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Agent Starting Run: bhkacelr with config:\n",
      "wandb: \tbatch_size: 64\n",
      "wandb: \tdropout: 0.2562022435438195\n",
      "wandb: \tepochs: 100\n",
      "wandb: \thidden_features: 256\n",
      "wandb: \tlap_pe_dim: 10\n",
      "wandb: \tlr: 1.0026445272229791\n",
      "wandb: \tnum_heads: 8\n",
      "wandb: \tnum_layers: 3\n",
      "wandb: \trwse_dim: 10\n",
      "wandb: \tweight_decay: 3.152617529864556e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\dadoi\\DataspellProjects\\Graph_Machine_Learning\\wandb\\run-20241201_162938-bhkacelr</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/runs/bhkacelr' target=\"_blank\">comfy-sweep-1</a></strong> to <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/sweeps/yw5hts1u' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/sweeps/yw5hts1u</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View sweep at <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/sweeps/yw5hts1u' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/sweeps/yw5hts1u</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/runs/bhkacelr' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/runs/bhkacelr</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dadoi\\AppData\\Local\\Temp\\ipykernel_30132\\314379181.py\", line 38, in train\n",
      "    loss.backward()\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\_tensor.py\", line 581, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py\", line 347, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\autograd\\graph.py\", line 825, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: CUDA error: an illegal memory access was encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">comfy-sweep-1</strong> at: <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/runs/bhkacelr' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/runs/bhkacelr</a><br/> View project at: <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241201_162938-bhkacelr\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run bhkacelr errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\wandb\\agents\\pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"C:\\Users\\dadoi\\AppData\\Local\\Temp\\ipykernel_30132\\314379181.py\", line 38, in train\n",
      "    loss.backward()\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\_tensor.py\", line 581, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py\", line 347, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\autograd\\graph.py\", line 825, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: CUDA error: an illegal memory access was encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "wandb: ERROR Run bhkacelr errored:\n",
      "wandb: ERROR Traceback (most recent call last):\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\wandb\\agents\\pyagent.py\", line 306, in _run_job\n",
      "wandb: ERROR     self._function()\n",
      "wandb: ERROR   File \"C:\\Users\\dadoi\\AppData\\Local\\Temp\\ipykernel_30132\\314379181.py\", line 38, in train\n",
      "wandb: ERROR     loss.backward()\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\_tensor.py\", line 581, in backward\n",
      "wandb: ERROR     torch.autograd.backward(\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py\", line 347, in backward\n",
      "wandb: ERROR     _engine_run_backward(\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\autograd\\graph.py\", line 825, in _engine_run_backward\n",
      "wandb: ERROR     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "wandb: ERROR RuntimeError: CUDA error: an illegal memory access was encountered\n",
      "wandb: ERROR CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "wandb: ERROR For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "wandb: ERROR Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "wandb: ERROR \n",
      "wandb: ERROR \n",
      "wandb: Agent Starting Run: aub6spcj with config:\n",
      "wandb: \tbatch_size: 16\n",
      "wandb: \tdropout: 0.05382400869788001\n",
      "wandb: \tepochs: 100\n",
      "wandb: \thidden_features: 256\n",
      "wandb: \tlap_pe_dim: 5\n",
      "wandb: \tlr: 1.0012534037488656\n",
      "wandb: \tnum_heads: 8\n",
      "wandb: \tnum_layers: 7\n",
      "wandb: \trwse_dim: 5\n",
      "wandb: \tweight_decay: 4.595001918182528e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\dadoi\\DataspellProjects\\Graph_Machine_Learning\\wandb\\run-20241201_163200-aub6spcj</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/runs/aub6spcj' target=\"_blank\">super-sweep-2</a></strong> to <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/sweeps/yw5hts1u' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/sweeps/yw5hts1u</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View sweep at <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/sweeps/yw5hts1u' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/sweeps/yw5hts1u</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/runs/aub6spcj' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/runs/aub6spcj</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dadoi\\AppData\\Local\\Temp\\ipykernel_30132\\314379181.py\", line 14, in train\n",
      "    ).to(device)\n",
      "      ^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "RuntimeError: CUDA error: an illegal memory access was encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">super-sweep-2</strong> at: <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/runs/aub6spcj' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/runs/aub6spcj</a><br/> View project at: <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241201_163200-aub6spcj\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run aub6spcj errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\wandb\\agents\\pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"C:\\Users\\dadoi\\AppData\\Local\\Temp\\ipykernel_30132\\314379181.py\", line 14, in train\n",
      "    ).to(device)\n",
      "      ^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "RuntimeError: CUDA error: an illegal memory access was encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "wandb: ERROR Run aub6spcj errored:\n",
      "wandb: ERROR Traceback (most recent call last):\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\wandb\\agents\\pyagent.py\", line 306, in _run_job\n",
      "wandb: ERROR     self._function()\n",
      "wandb: ERROR   File \"C:\\Users\\dadoi\\AppData\\Local\\Temp\\ipykernel_30132\\314379181.py\", line 14, in train\n",
      "wandb: ERROR     ).to(device)\n",
      "wandb: ERROR       ^^^^^^^^^^\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1340, in to\n",
      "wandb: ERROR     return self._apply(convert)\n",
      "wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 900, in _apply\n",
      "wandb: ERROR     module._apply(fn)\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 927, in _apply\n",
      "wandb: ERROR     param_applied = fn(param)\n",
      "wandb: ERROR                     ^^^^^^^^^\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1326, in convert\n",
      "wandb: ERROR     return t.to(\n",
      "wandb: ERROR            ^^^^^\n",
      "wandb: ERROR RuntimeError: CUDA error: an illegal memory access was encountered\n",
      "wandb: ERROR CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "wandb: ERROR For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "wandb: ERROR Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "wandb: ERROR \n",
      "wandb: ERROR \n",
      "wandb: Agent Starting Run: fcmq9yuu with config:\n",
      "wandb: \tbatch_size: 16\n",
      "wandb: \tdropout: 0.18791144475443683\n",
      "wandb: \tepochs: 100\n",
      "wandb: \thidden_features: 64\n",
      "wandb: \tlap_pe_dim: 15\n",
      "wandb: \tlr: 1.0060610349052628\n",
      "wandb: \tnum_heads: 8\n",
      "wandb: \tnum_layers: 7\n",
      "wandb: \trwse_dim: 10\n",
      "wandb: \tweight_decay: 2.0047726456282e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\dadoi\\DataspellProjects\\Graph_Machine_Learning\\wandb\\run-20241201_163205-fcmq9yuu</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/runs/fcmq9yuu' target=\"_blank\">smooth-sweep-3</a></strong> to <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/sweeps/yw5hts1u' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/sweeps/yw5hts1u</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View sweep at <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/sweeps/yw5hts1u' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/sweeps/yw5hts1u</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/runs/fcmq9yuu' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/runs/fcmq9yuu</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dadoi\\AppData\\Local\\Temp\\ipykernel_30132\\314379181.py\", line 14, in train\n",
      "    ).to(device)\n",
      "      ^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "RuntimeError: CUDA error: an illegal memory access was encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">smooth-sweep-3</strong> at: <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/runs/fcmq9yuu' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/runs/fcmq9yuu</a><br/> View project at: <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241201_163205-fcmq9yuu\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run fcmq9yuu errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\wandb\\agents\\pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"C:\\Users\\dadoi\\AppData\\Local\\Temp\\ipykernel_30132\\314379181.py\", line 14, in train\n",
      "    ).to(device)\n",
      "      ^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "RuntimeError: CUDA error: an illegal memory access was encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "wandb: ERROR Run fcmq9yuu errored:\n",
      "wandb: ERROR Traceback (most recent call last):\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\wandb\\agents\\pyagent.py\", line 306, in _run_job\n",
      "wandb: ERROR     self._function()\n",
      "wandb: ERROR   File \"C:\\Users\\dadoi\\AppData\\Local\\Temp\\ipykernel_30132\\314379181.py\", line 14, in train\n",
      "wandb: ERROR     ).to(device)\n",
      "wandb: ERROR       ^^^^^^^^^^\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1340, in to\n",
      "wandb: ERROR     return self._apply(convert)\n",
      "wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 900, in _apply\n",
      "wandb: ERROR     module._apply(fn)\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 927, in _apply\n",
      "wandb: ERROR     param_applied = fn(param)\n",
      "wandb: ERROR                     ^^^^^^^^^\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1326, in convert\n",
      "wandb: ERROR     return t.to(\n",
      "wandb: ERROR            ^^^^^\n",
      "wandb: ERROR RuntimeError: CUDA error: an illegal memory access was encountered\n",
      "wandb: ERROR CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "wandb: ERROR For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "wandb: ERROR Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "wandb: ERROR \n",
      "wandb: ERROR \n",
      "wandb: Sweep Agent: Waiting for job.\n",
      "wandb: Job received.\n",
      "wandb: Agent Starting Run: yiyk8wtm with config:\n",
      "wandb: \tbatch_size: 32\n",
      "wandb: \tdropout: 0.271149978092737\n",
      "wandb: \tepochs: 100\n",
      "wandb: \thidden_features: 64\n",
      "wandb: \tlap_pe_dim: 15\n",
      "wandb: \tlr: 1.0005869736681423\n",
      "wandb: \tnum_heads: 8\n",
      "wandb: \tnum_layers: 7\n",
      "wandb: \trwse_dim: 10\n",
      "wandb: \tweight_decay: 1.88917666259248e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\dadoi\\DataspellProjects\\Graph_Machine_Learning\\wandb\\run-20241201_163220-yiyk8wtm</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/runs/yiyk8wtm' target=\"_blank\">smart-sweep-4</a></strong> to <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/sweeps/yw5hts1u' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/sweeps/yw5hts1u</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View sweep at <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/sweeps/yw5hts1u' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/sweeps/yw5hts1u</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/runs/yiyk8wtm' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/runs/yiyk8wtm</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dadoi\\AppData\\Local\\Temp\\ipykernel_30132\\314379181.py\", line 14, in train\n",
      "    ).to(device)\n",
      "      ^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "RuntimeError: CUDA error: an illegal memory access was encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">smart-sweep-4</strong> at: <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/runs/yiyk8wtm' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/runs/yiyk8wtm</a><br/> View project at: <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241201_163220-yiyk8wtm\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run yiyk8wtm errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\wandb\\agents\\pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"C:\\Users\\dadoi\\AppData\\Local\\Temp\\ipykernel_30132\\314379181.py\", line 14, in train\n",
      "    ).to(device)\n",
      "      ^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "RuntimeError: CUDA error: an illegal memory access was encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "wandb: ERROR Run yiyk8wtm errored:\n",
      "wandb: ERROR Traceback (most recent call last):\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\wandb\\agents\\pyagent.py\", line 306, in _run_job\n",
      "wandb: ERROR     self._function()\n",
      "wandb: ERROR   File \"C:\\Users\\dadoi\\AppData\\Local\\Temp\\ipykernel_30132\\314379181.py\", line 14, in train\n",
      "wandb: ERROR     ).to(device)\n",
      "wandb: ERROR       ^^^^^^^^^^\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1340, in to\n",
      "wandb: ERROR     return self._apply(convert)\n",
      "wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 900, in _apply\n",
      "wandb: ERROR     module._apply(fn)\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 927, in _apply\n",
      "wandb: ERROR     param_applied = fn(param)\n",
      "wandb: ERROR                     ^^^^^^^^^\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1326, in convert\n",
      "wandb: ERROR     return t.to(\n",
      "wandb: ERROR            ^^^^^\n",
      "wandb: ERROR RuntimeError: CUDA error: an illegal memory access was encountered\n",
      "wandb: ERROR CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "wandb: ERROR For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "wandb: ERROR Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "wandb: ERROR \n",
      "wandb: ERROR \n",
      "wandb: Agent Starting Run: a7wo5lji with config:\n",
      "wandb: \tbatch_size: 32\n",
      "wandb: \tdropout: 0.1076061848751495\n",
      "wandb: \tepochs: 100\n",
      "wandb: \thidden_features: 64\n",
      "wandb: \tlap_pe_dim: 15\n",
      "wandb: \tlr: 1.0051604138735415\n",
      "wandb: \tnum_heads: 8\n",
      "wandb: \tnum_layers: 5\n",
      "wandb: \trwse_dim: 5\n",
      "wandb: \tweight_decay: 6.56335891318391e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\dadoi\\DataspellProjects\\Graph_Machine_Learning\\wandb\\run-20241201_163226-a7wo5lji</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/runs/a7wo5lji' target=\"_blank\">rich-sweep-5</a></strong> to <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/sweeps/yw5hts1u' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/sweeps/yw5hts1u</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View sweep at <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/sweeps/yw5hts1u' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/sweeps/yw5hts1u</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/runs/a7wo5lji' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/runs/a7wo5lji</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dadoi\\AppData\\Local\\Temp\\ipykernel_30132\\314379181.py\", line 14, in train\n",
      "    ).to(device)\n",
      "      ^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "RuntimeError: CUDA error: an illegal memory access was encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rich-sweep-5</strong> at: <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/runs/a7wo5lji' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/runs/a7wo5lji</a><br/> View project at: <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241201_163226-a7wo5lji\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run a7wo5lji errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\wandb\\agents\\pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"C:\\Users\\dadoi\\AppData\\Local\\Temp\\ipykernel_30132\\314379181.py\", line 14, in train\n",
      "    ).to(device)\n",
      "      ^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "RuntimeError: CUDA error: an illegal memory access was encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "wandb: ERROR Run a7wo5lji errored:\n",
      "wandb: ERROR Traceback (most recent call last):\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\wandb\\agents\\pyagent.py\", line 306, in _run_job\n",
      "wandb: ERROR     self._function()\n",
      "wandb: ERROR   File \"C:\\Users\\dadoi\\AppData\\Local\\Temp\\ipykernel_30132\\314379181.py\", line 14, in train\n",
      "wandb: ERROR     ).to(device)\n",
      "wandb: ERROR       ^^^^^^^^^^\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1340, in to\n",
      "wandb: ERROR     return self._apply(convert)\n",
      "wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 900, in _apply\n",
      "wandb: ERROR     module._apply(fn)\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 927, in _apply\n",
      "wandb: ERROR     param_applied = fn(param)\n",
      "wandb: ERROR                     ^^^^^^^^^\n",
      "wandb: ERROR   File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1326, in convert\n",
      "wandb: ERROR     return t.to(\n",
      "wandb: ERROR            ^^^^^\n",
      "wandb: ERROR RuntimeError: CUDA error: an illegal memory access was encountered\n",
      "wandb: ERROR CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "wandb: ERROR For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "wandb: ERROR Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "wandb: ERROR \n",
      "wandb: ERROR \n",
      "wandb: Agent Starting Run: y873m9gt with config:\n",
      "wandb: \tbatch_size: 64\n",
      "wandb: \tdropout: 0.21276450125679872\n",
      "wandb: \tepochs: 100\n",
      "wandb: \thidden_features: 128\n",
      "wandb: \tlap_pe_dim: 10\n",
      "wandb: \tlr: 1.0010511055113027\n",
      "wandb: \tnum_heads: 4\n",
      "wandb: \tnum_layers: 7\n",
      "wandb: \trwse_dim: 10\n",
      "wandb: \tweight_decay: 5.472271621673161e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\dadoi\\DataspellProjects\\Graph_Machine_Learning\\wandb\\run-20241201_163232-y873m9gt</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/runs/y873m9gt' target=\"_blank\">stoic-sweep-6</a></strong> to <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/sweeps/yw5hts1u' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/sweeps/yw5hts1u</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View sweep at <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/sweeps/yw5hts1u' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/sweeps/yw5hts1u</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/runs/y873m9gt' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/runs/y873m9gt</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dadoi\\AppData\\Local\\Temp\\ipykernel_30132\\314379181.py\", line 14, in train\n",
      "    ).to(device)\n",
      "      ^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "RuntimeError: CUDA error: an illegal memory access was encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stoic-sweep-6</strong> at: <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/runs/y873m9gt' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning/runs/y873m9gt</a><br/> View project at: <a href='https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning' target=\"_blank\">https://wandb.ai/mak84271/peptides-func-hyperparameter-tuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241201_163232-y873m9gt\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T14:00:09.068131Z",
     "start_time": "2024-12-02T13:59:49.181572Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "10150d70055e40b9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.dropbox.com/s/ycsq37q8sxs1ou8/peptidesfunc.zip?dl=1\n",
      "Extracting path\\to\\data\\peptidesfunc.zip\n",
      "Processing...\n",
      "Processing train dataset: 100%|| 10873/10873 [00:00<00:00, 42121.36it/s]\n",
      "Processing val dataset: 100%|| 2331/2331 [00:00<00:00, 31418.36it/s]\n",
      "Processing test dataset: 100%|| 2331/2331 [00:00<00:00, 55873.88it/s]\n",
      "Done!\n",
      "C:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found dtype Long but expected Float",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 47\u001B[0m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;66;03m# Run training\u001B[39;00m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m100\u001B[39m):\n\u001B[1;32m---> 47\u001B[0m     train()\n",
      "Cell \u001B[1;32mIn[1], line 40\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m()\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m loader:\n\u001B[0;32m     39\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 40\u001B[0m     out \u001B[38;5;241m=\u001B[39m model(data\u001B[38;5;241m.\u001B[39mx, data\u001B[38;5;241m.\u001B[39medge_index, data\u001B[38;5;241m.\u001B[39mbatch)\n\u001B[0;32m     41\u001B[0m     loss \u001B[38;5;241m=\u001B[39m criterion(out, data\u001B[38;5;241m.\u001B[39my)\n\u001B[0;32m     42\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32mC:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[1], line 15\u001B[0m, in \u001B[0;36mPeptidesFuncGCN.forward\u001B[1;34m(self, x, edge_index, batch)\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, edge_index, batch):\n\u001B[1;32m---> 15\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv1(x, edge_index)\u001B[38;5;241m.\u001B[39mrelu()\n\u001B[0;32m     16\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv2(x, edge_index)\u001B[38;5;241m.\u001B[39mrelu()\n\u001B[0;32m     17\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv3(x, edge_index)\u001B[38;5;241m.\u001B[39mrelu()\n",
      "File \u001B[1;32mC:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mC:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:241\u001B[0m, in \u001B[0;36mGCNConv.forward\u001B[1;34m(self, x, edge_index, edge_weight)\u001B[0m\n\u001B[0;32m    239\u001B[0m cache \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cached_edge_index\n\u001B[0;32m    240\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 241\u001B[0m     edge_index, edge_weight \u001B[38;5;241m=\u001B[39m gcn_norm(  \u001B[38;5;66;03m# yapf: disable\u001B[39;00m\n\u001B[0;32m    242\u001B[0m         edge_index, edge_weight, x\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnode_dim),\n\u001B[0;32m    243\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimproved, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_self_loops, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mflow, x\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[0;32m    244\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcached:\n\u001B[0;32m    245\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cached_edge_index \u001B[38;5;241m=\u001B[39m (edge_index, edge_weight)\n",
      "File \u001B[1;32mC:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:109\u001B[0m, in \u001B[0;36mgcn_norm\u001B[1;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001B[0m\n\u001B[0;32m    107\u001B[0m idx \u001B[38;5;241m=\u001B[39m col \u001B[38;5;28;01mif\u001B[39;00m flow \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msource_to_target\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m row\n\u001B[0;32m    108\u001B[0m deg \u001B[38;5;241m=\u001B[39m scatter(edge_weight, idx, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, dim_size\u001B[38;5;241m=\u001B[39mnum_nodes, reduce\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msum\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 109\u001B[0m deg_inv_sqrt \u001B[38;5;241m=\u001B[39m deg\u001B[38;5;241m.\u001B[39mpow_(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m0.5\u001B[39m)\n\u001B[0;32m    110\u001B[0m deg_inv_sqrt\u001B[38;5;241m.\u001B[39mmasked_fill_(deg_inv_sqrt \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minf\u001B[39m\u001B[38;5;124m'\u001B[39m), \u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m    111\u001B[0m edge_weight \u001B[38;5;241m=\u001B[39m deg_inv_sqrt[row] \u001B[38;5;241m*\u001B[39m edge_weight \u001B[38;5;241m*\u001B[39m deg_inv_sqrt[col]\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Found dtype Long but expected Float"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a0c68e0cc1a08264"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
