{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T10:42:45.557585Z",
     "start_time": "2025-01-15T10:42:39.877322Z"
    }
   },
   "source": [
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "from hik.data.kitchen import Kitchen\n",
    "from hik.data import PersonSequences\n",
    "from hik.vis import plot_pose\n",
    "from networkx.classes import nodes"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T10:43:00.197532Z",
     "start_time": "2025-01-15T10:42:45.566746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = \"A\"\n",
    "# load geometry\n",
    "kitchen = Kitchen.load_for_dataset(\n",
    "    dataset=dataset,\n",
    "    data_location=\"data/scenes\"\n",
    ")\n",
    "\n",
    "# load poses\n",
    "person_seqs = PersonSequences(\n",
    "    person_path=\"data/poses\"\n",
    ")\n",
    "\n",
    "smplx_path = \"data/body_models\"\n"
   ],
   "id": "3d1611f2822b992",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 319/319 [00:14<00:00, 21.96it/s]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T10:43:00.242211Z",
     "start_time": "2025-01-15T10:43:00.237511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example skeleton edges for the 29 joints:\n",
    "# Each tuple (i, j) indicates a bidirectional edge between joints i and j.\n",
    "# Here, we just create a dummy list. Replace with your actual skeleton edges.\n",
    "SKELETON_29 = [\n",
    "    (0,1), (1,2), (2,3), (3,4), # e.g. a chain\n",
    "    (2,5), (5,6), (6,7),        # branching, etc...\n",
    "    # ...\n",
    "]\n",
    "\n",
    "def create_adjacency_29(num_joints=29):\n",
    "    A = np.zeros((num_joints, num_joints), dtype=np.float32)\n",
    "    for (i, j) in SKELETON_29:\n",
    "        A[i, j] = 1.0\n",
    "        A[j, i] = 1.0\n",
    "    return A\n",
    "\n",
    "A1 = create_adjacency_29()\n"
   ],
   "id": "3cd7802221b2d486",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<hik.data.person_sequence.PersonSequences at 0x78c6dcbbeca0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# group_s2_indices is a list of lists, where each sub-list\n",
    "# is the s1-joints that map to a single s2 node.\n",
    "group_s2_indices = [\n",
    "    [0,1],      # node 0 in s2 is the average of joints 0,1 in s1\n",
    "    [2,3,4],    # node 1 is avg of s1 joints 2,3,4\n",
    "    [5,6,7],    # node 2 is avg of s1 joints 5,6,7\n",
    "    # ...\n",
    "]\n",
    "\n",
    "def group_poses(poses3d: np.ndarray, grouping: list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param poses3d: shape (T, 29, 3) or (N, 29, 3)\n",
    "    :param grouping: list of lists; each sub-list is the s1-joint indices to average\n",
    "    :return: shape (T, #groups, 3)\n",
    "    \"\"\"\n",
    "    # If input is (T, 29, 3), just do a loop over grouping\n",
    "    T = poses3d.shape[0]\n",
    "    num_groups = len(grouping)\n",
    "    out = np.zeros((T, num_groups, 3), dtype=np.float32)\n",
    "    for g_idx, grp in enumerate(grouping):\n",
    "        out[:, g_idx] = np.mean(poses3d[:, grp, :], axis=1)\n",
    "    return out\n",
    "\n",
    "def create_adjacency_for_s2(grouping):\n",
    "    \"\"\"\n",
    "    Build adjacency for the s2 scale.\n",
    "    It's up to you how you define edges among these coarser parts.\n",
    "    \"\"\"\n",
    "    num_parts = len(grouping)\n",
    "    A2 = np.zeros((num_parts, num_parts), dtype=np.float32)\n",
    "    # Example: connect them in some chain or some anatomically meaningful structure\n",
    "    # For demonstration, let's just connect consecutive group indices\n",
    "    for i in range(num_parts-1):\n",
    "        A2[i, i+1] = 1.0\n",
    "        A2[i+1, i] = 1.0\n",
    "    return A2\n",
    "\n",
    "A2 = create_adjacency_for_s2(group_s2_indices)\n"
   ],
   "id": "281e4ca624ff94e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def slice_sequence(\n",
    "        poses3d: np.ndarray,\n",
    "        window_in: int = 25,\n",
    "        window_out: int = 10\n",
    "):\n",
    "    \"\"\"\n",
    "    :param poses3d: shape (T, 29, 3)\n",
    "    :param window_in: number of past frames used as input\n",
    "    :param window_out: number of future frames to predict\n",
    "    :return: a list of (input_poses, target_poses), each with shape (window_in, 29, 3) or (window_out, 29, 3)\n",
    "    \"\"\"\n",
    "    T = poses3d.shape[0]\n",
    "    samples = []\n",
    "    # e.g. for t in range(0, T-window_in-window_out):\n",
    "    for start in range(0, T - window_in - window_out + 1):\n",
    "        in_poses = poses3d[start : start + window_in]\n",
    "        out_poses = poses3d[start + window_in : start + window_in + window_out]\n",
    "        samples.append((in_poses, out_poses))\n",
    "    return samples\n"
   ],
   "id": "d236a7c0da4faac3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "class MotionPredictionDataset(data.Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            person_seqs: PersonSequences,  # your loaded PersonSequences\n",
    "            dataset_name: str,\n",
    "            window_in=25,\n",
    "            window_out=10,\n",
    "            transform=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param person_seqs: the PersonSequences object\n",
    "        :param dataset_name: which dataset key to fetch from person_seqs\n",
    "        :param window_in: how many frames to use as 'past'\n",
    "        :param window_out: how many frames to predict\n",
    "        :param transform: optional transform function for data augmentation, etc.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.person_seqs = person_seqs\n",
    "        self.dataset_name = dataset_name\n",
    "        self.window_in = window_in\n",
    "        self.window_out = window_out\n",
    "        self.transform = transform\n",
    "\n",
    "        # gather all PersonSequence objects for the chosen dataset\n",
    "        self.sequences = person_seqs.get_sequences(dataset_name)\n",
    "\n",
    "        # create a list of all (input, target) pairs across all sequences\n",
    "        self.samples = []  # will hold tuples: (in_poses, out_poses)\n",
    "        for seq in self.sequences:\n",
    "            poses3d = seq.poses3d  # shape (T, 29, 3)\n",
    "            # get slices\n",
    "            seq_slices = slice_sequence(\n",
    "                poses3d, window_in=window_in, window_out=window_out\n",
    "            )\n",
    "            # store them\n",
    "            for (in_poses, out_poses) in seq_slices:\n",
    "                self.samples.append((in_poses, out_poses))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        in_poses, out_poses = self.samples[index]  # shape => (window_in, 29, 3), (window_out, 29, 3)\n",
    "\n",
    "        # Optional: multi-scale\n",
    "        in_poses_s2 = group_poses(in_poses[None], group_s2_indices).squeeze(0)\n",
    "        #   => shape (window_in, #groups_s2, 3)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            in_poses, out_poses = self.transform(in_poses, out_poses)\n",
    "\n",
    "        # convert to torch tensors (batch dimension can be added in collate)\n",
    "        in_poses = torch.from_numpy(in_poses).float()  # => shape [window_in, 29, 3]\n",
    "        in_poses_s2 = torch.from_numpy(in_poses_s2).float()  # => e.g. [window_in, 9, 3], depends on grouping\n",
    "        out_poses = torch.from_numpy(out_poses).float()\n",
    "\n",
    "        return {\n",
    "            \"in_s1\": in_poses,      # (window_in, 29, 3)\n",
    "            \"in_s2\": in_poses_s2,   # (window_in, #groups_s2, 3)\n",
    "            \"target\": out_poses,    # (window_out, 29, 3)\n",
    "        }\n"
   ],
   "id": "81c0c7be3e9e2332"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def motion_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch is a list of dicts, each from __getitem__.\n",
    "    We'll transform them into batched tensors, e.g. [B, T, 29, 3].\n",
    "    \"\"\"\n",
    "    in_s1_list  = [sample[\"in_s1\"] for sample in batch]   # each (window_in, 29, 3)\n",
    "    in_s2_list  = [sample[\"in_s2\"] for sample in batch]\n",
    "    target_list = [sample[\"target\"] for sample in batch]\n",
    "\n",
    "    # stack them\n",
    "    in_s1  = torch.stack(in_s1_list, dim=0)    # => [B, window_in, 29, 3]\n",
    "    in_s2  = torch.stack(in_s2_list, dim=0)    # => [B, window_in, #groups_s2, 3]\n",
    "    target = torch.stack(target_list, dim=0)   # => [B, window_out, 29, 3]\n",
    "\n",
    "    return in_s1, in_s2, target\n"
   ],
   "id": "93d4a20c4c093247"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Suppose you have your PersonSequences loaded:\n",
    "person_seqs = PersonSequences(person_path=\"data/poses\")\n",
    "\n",
    "# Instantiate your dataset:\n",
    "train_dataset = MotionPredictionDataset(\n",
    "    person_seqs=person_seqs,\n",
    "    dataset_name=\"A\",      # or \"B\" / \"C\" / ...\n",
    "    window_in=25,\n",
    "    window_out=10\n",
    ")\n",
    "\n",
    "# Create a DataLoader:\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0,   # or >0 if you want multiprocessing\n",
    "    collate_fn=motion_collate_fn\n",
    ")\n",
    "\n",
    "# Now you can iterate:\n",
    "for batch_idx, (in_s1, in_s2, target) in enumerate(train_loader):\n",
    "    # in_s1: [B, 25, 29, 3]\n",
    "    # in_s2: [B, 25, #groups_s2, 3]\n",
    "    # target: [B, 10, 29, 3]\n",
    "\n",
    "    # possibly rearrange if your model wants [B, 3, T, N]\n",
    "    in_s1 = in_s1.permute(0, 3, 1, 2)  # => [B, 3, 25, 29]\n",
    "    # likewise for in_s2\n",
    "\n",
    "    # pass to your model, e.g. model(in_s1, in_s2)\n",
    "    # ...\n",
    "    pass\n"
   ],
   "id": "3b52856f2d650387"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
